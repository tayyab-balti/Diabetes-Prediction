


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


df = pd.read_csv("diabetes.csv")
df





df.shape


df.info()


df.describe()





# Handle missing values
df.isnull().sum()


# Converting 0 to nan bcz BP, Glucose, etc couldn't be zero
df[["Glucose", "BloodPressure",	"SkinThickness", "Insulin", "BMI"]] = df[["Glucose", "BloodPressure",	"SkinThickness", "Insulin", "BMI"]].replace(0, np.nan)


df.isnull().sum()


df["Glucose"] = df["Glucose"].fillna(df["Glucose"].mean())
df["BloodPressure"] = df["BloodPressure"].fillna(df["BloodPressure"].mean())
df["SkinThickness"] = df["SkinThickness"].fillna(df["SkinThickness"].mean())
df["Insulin"] = df["Insulin"].fillna(df["Insulin"].mean())
df["BMI"] = df["BMI"].fillna(df["BMI"].mean())


df.isnull().sum()


# Normalize the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df.drop(columns=['Outcome']))





# show distribution of a numerical feature

# df['Glucose'].hist()
# df['Insulin'].hist()
df['Age'].hist()


# show the correlation between features (like how Glucose and Outcome are related)

sns.boxplot(x='Outcome', y='Glucose', data=df)
plt.title('Glucose Levels in Diabetic vs. Non-Diabetic Individuals')


# Check how features are correlated with each other

sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation between Features')


# relationship bw two numerical features
sns.scatterplot(x='Glucose', y='Insulin', hue='Outcome', data=df)


# show relationship bw all pairs of features
sns.pairplot(df, hue="Outcome")





X = df.drop(columns=["Outcome"], axis=1)
X


y = df["Outcome"]
y





# to know which features are more related to Label
from sklearn.feature_selection import SelectKBest, chi2


k = 5
chi2_selector = SelectKBest(chi2, k=k)
X_new = chi2_selector.fit_transform(X, y)


selected_features_indices = chi2_selector.get_support(indices=True)
print(selected_features_indices)


selected_features = X.columns[selected_features_indices]
print(selected_features)


selected_features_df = df[selected_features]
selected_features_df


selected_features_df['Outcome'] = y





x1 = selected_features_df.drop(["Outcome"], axis=1)
y1 = selected_features_df["Outcome"]


x1.shape


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(x1, y1, test_size= 0.2, random_state= 42)





# Use top classification models to know which is best for this particular dataset
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

from sklearn.pipeline import Pipeline


# output of StandScaler() would be input of LogisticRegression() and so on...

pipeline_lr = Pipeline([('scaler1', StandardScaler()), 
                        ('lr_classifier', LogisticRegression())
                        ])

pipeline_knn = Pipeline([('scaler2', StandardScaler()), 
                         ('knn_classifier', KNeighborsClassifier())
                        ])

pipeline_svc = Pipeline([('scaler3', StandardScaler()), 
                         ('svc_classifier', SVC())
                        ])

pipeline_dt = Pipeline([('scaler4', StandardScaler()), 
                        ('dt_classifier', DecisionTreeClassifier())
                        ])

pipeline_rf = Pipeline([('scaler5', StandardScaler()), 
                        ('rf_classifier', RandomForestClassifier())
                        ])

pipeline_gb = Pipeline([('scaler6', StandardScaler()), 
                        ('gb_classifier', GaussianNB())
                        ])


pipelines = [
    pipeline_lr,
    pipeline_knn,
    pipeline_svc,
    pipeline_dt,
    pipeline_rf,
    pipeline_gb
]


pipe_dict = {
    0: "LR",
    1: "KNN",
    2: "SVM",
    3: "DT",
    4: "RF",
    5: "GB"
}
pipe_dict


for i, model in enumerate(pipelines):
    # Fit each model
    model.fit(X_train, y_train)
    
    # Get predictions
    y_pred = model.predict(X_test)
    
    # Print the test accuracy
    print(f"{pipe_dict[i]} Test Accuracy: {model.score(X_test, y_test)*100}")





model_names = ["Logistic Regression", "KNN", "SVM", "Decision Trees", "Random Forest", "Naive Bayes"]
accuracies = [77.27, 70.12, 73.13, 71.42, 72.72, 74.67]


# Create a bar chart
plt.figure(figsize=(10, 6))
plt.barh(model_names, accuracies, color='skyblue')
plt.title("Model Accuracy Comparison")
plt.xlabel("Accuracy (%)")

# Save the plot as a png file
plt.savefig("model_accuracy.png")

plt.show()





from sklearn.linear_model import LogisticRegression


lr = LogisticRegression()


lr.fit(X_train, y_train)


y_pred = lr.predict(X_test)


from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)*100
accuracy








from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


cm = confusion_matrix(y_test, y_pred)


disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()





from sklearn.model_selection import KFold, cross_val_score


from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Scaling the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply logistic regression
model = LogisticRegression(max_iter=500)
model.fit(X_train_scaled, y_train)



model = LogisticRegression(max_iter=500)

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

cv_scores = cross_val_score(model, X, y, cv=kfold, scoring="accuracy")

print("Cross-validation scores:", cv_scores*100)
print("Mean CV score:", cv_scores.mean()*100)



